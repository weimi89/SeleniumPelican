#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Performance test runner for SeleniumPelican
Executes performance tests and generates comprehensive reports
"""

import argparse
import subprocess
import sys
from datetime import datetime
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.core.logging_config import get_logger


def run_performance_tests(
    test_type: str = "all", save_baseline: bool = False, headless: bool = True
):
    """
    Run performance tests with specified options

    Args:
        test_type: Type of tests to run ('all', 'browser', 'scraper', 'memory')
        save_baseline: Whether to save results as new baseline
        headless: Whether to run tests in headless mode
    """
    logger = get_logger("performance_runner")
    logger.info("ğŸš€ å•Ÿå‹• SeleniumPelican æ•ˆèƒ½æ¸¬è©¦", test_type=test_type, headless=headless)

    # Prepare pytest command
    pytest_args = [
        "python",
        "-m",
        "pytest",
        "tests/performance/",
        "-v",
        "--tb=short",
        "-x",  # Stop on first failure
    ]

    # Add test type specific filters
    if test_type == "browser":
        pytest_args.extend(["-k", "browser"])
    elif test_type == "scraper":
        pytest_args.extend(["-k", "scraper"])
    elif test_type == "memory":
        pytest_args.extend(["-k", "memory"])

    # Add headless flag as environment variable
    env = {"PYTEST_HEADLESS": "true" if headless else "false"}

    try:
        # Run the tests
        logger.info("ğŸ“Š åŸ·è¡Œæ•ˆèƒ½æ¸¬è©¦...", command=" ".join(pytest_args))
        result = subprocess.run(pytest_args, capture_output=True, text=True, env=env)

        # Log results
        if result.returncode == 0:
            logger.log_operation_success(
                "æ•ˆèƒ½æ¸¬è©¦åŸ·è¡Œ", test_type=test_type, exit_code=result.returncode
            )
            print("âœ… æ‰€æœ‰æ•ˆèƒ½æ¸¬è©¦é€šé")
        else:
            logger.warning(
                "éƒ¨åˆ†æ•ˆèƒ½æ¸¬è©¦å¤±æ•—",
                test_type=test_type,
                exit_code=result.returncode,
                stderr=result.stderr[:500],  # Limit error output
            )
            print("âš ï¸ éƒ¨åˆ†æ•ˆèƒ½æ¸¬è©¦å¤±æ•—")

        # Show output
        if result.stdout:
            print("\n" + "=" * 60)
            print("æ¸¬è©¦è¼¸å‡º:")
            print("=" * 60)
            print(result.stdout)

        if result.stderr and result.returncode != 0:
            print("\n" + "=" * 60)
            print("éŒ¯èª¤è¼¸å‡º:")
            print("=" * 60)
            print(result.stderr)

        return result.returncode == 0

    except Exception as e:
        logger.error(f"âŒ æ•ˆèƒ½æ¸¬è©¦åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        return False


def generate_performance_summary():
    """Generate a summary of all performance test results"""
    logger = get_logger("performance_summary")

    reports_dir = Path("reports/performance")
    if not reports_dir.exists():
        logger.warning("âŒ æ²’æœ‰æ‰¾åˆ°æ•ˆèƒ½æ¸¬è©¦å ±å‘Šç›®éŒ„")
        return

    # Find the latest performance report
    report_files = list(reports_dir.glob("performance_report_*.txt"))
    if not report_files:
        logger.warning("âŒ æ²’æœ‰æ‰¾åˆ°æ•ˆèƒ½æ¸¬è©¦å ±å‘Š")
        return

    latest_report = max(report_files, key=lambda f: f.stat().st_mtime)

    logger.info(f"ğŸ“Š æœ€æ–°æ•ˆèƒ½å ±å‘Š: {latest_report}")

    # Read and display the report
    try:
        with open(latest_report, "r", encoding="utf-8") as f:
            report_content = f.read()

        print("\n" + "=" * 80)
        print("æ•ˆèƒ½æ¸¬è©¦æ‘˜è¦å ±å‘Š")
        print("=" * 80)
        print(report_content)

        # Also check for baseline comparison
        baselines_file = Path("tests/performance/baselines.json")
        if baselines_file.exists():
            logger.info(f"ğŸ“ˆ åŸºæº–æ•¸æ“šæ–‡ä»¶: {baselines_file}")
        else:
            logger.info("ğŸ’¡ å»ºè­°åŸ·è¡Œ --save-baseline å»ºç«‹æ•ˆèƒ½åŸºæº–")

    except Exception as e:
        logger.error(f"âŒ è®€å–æ•ˆèƒ½å ±å‘Šå¤±æ•—: {e}")


def check_performance_requirements():
    """Check if current performance meets requirements"""
    logger = get_logger("performance_check")

    # Define performance requirements based on OpenSpec
    requirements = {
        "browser_startup_headless": {"max_time": 10.0, "unit": "seconds"},
        "browser_startup_windowed": {"max_time": 15.0, "unit": "seconds"},
        "login_operation": {"max_time": 30.0, "unit": "seconds"},
        "navigation_operation": {"max_time": 10.0, "unit": "seconds"},
        "max_memory_usage": {"max_value": 500.0, "unit": "MB"},
        "max_degradation": {"max_percent": 50.0, "unit": "percent"},
    }

    logger.info("ğŸ“‹ æ•ˆèƒ½éœ€æ±‚æª¢æŸ¥", requirements=requirements)

    # This would analyze recent test results against requirements
    # For now, just log the requirements
    print("\nğŸ“‹ æ•ˆèƒ½éœ€æ±‚æ¨™æº–:")
    print("=" * 50)
    for operation, req in requirements.items():
        if "max_time" in req:
            print(f"â€¢ {operation}: â‰¤ {req['max_time']} {req['unit']}")
        elif "max_value" in req:
            print(f"â€¢ {operation}: â‰¤ {req['max_value']} {req['unit']}")
        elif "max_percent" in req:
            print(f"â€¢ {operation}: â‰¤ {req['max_percent']}% degradation")

    print("\nğŸ’¡ åŸ·è¡Œ 'python scripts/run_performance_tests.py' ä¾†é©—è­‰æ•ˆèƒ½")


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(description="SeleniumPelican æ•ˆèƒ½æ¸¬è©¦åŸ·è¡Œå™¨")

    parser.add_argument(
        "--type",
        choices=["all", "browser", "scraper", "memory"],
        default="all",
        help="æ¸¬è©¦é¡å‹ (é è¨­: all)",
    )

    parser.add_argument("--save-baseline", action="store_true", help="å°‡æ¸¬è©¦çµæœä¿å­˜ç‚ºæ–°çš„åŸºæº–")

    parser.add_argument("--windowed", action="store_true", help="ä½¿ç”¨è¦–çª—æ¨¡å¼é‹è¡Œæ¸¬è©¦ (é è¨­: ç„¡é ­æ¨¡å¼)")

    parser.add_argument("--summary", action="store_true", help="åªé¡¯ç¤ºæœ€æ–°çš„æ•ˆèƒ½æ‘˜è¦å ±å‘Š")

    parser.add_argument("--check-requirements", action="store_true", help="æª¢æŸ¥æ•ˆèƒ½éœ€æ±‚æ¨™æº–")

    args = parser.parse_args()

    if args.check_requirements:
        check_performance_requirements()
        return

    if args.summary:
        generate_performance_summary()
        return

    # Run performance tests
    success = run_performance_tests(
        test_type=args.type,
        save_baseline=args.save_baseline,
        headless=not args.windowed,
    )

    # Generate summary after tests
    if success:
        print("\n" + "=" * 60)
        print("æ­£åœ¨ç”Ÿæˆæ•ˆèƒ½æ‘˜è¦...")
        generate_performance_summary()

        if args.save_baseline:
            print("âœ… æ–°çš„æ•ˆèƒ½åŸºæº–å·²ä¿å­˜")

    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
