#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ—¥èªŒåˆ†æå’Œç›£æ§æ©Ÿåˆ¶

æä¾›çµæ§‹åŒ–æ—¥èªŒåˆ†æã€æ¨¡å¼æª¢æ¸¬ã€ç•°å¸¸ç›£æ§å’Œæ€§èƒ½è¿½è¹¤åŠŸèƒ½ã€‚
æ”¯æ´å³æ™‚ç›£æ§å’Œæ­·å²åˆ†æã€‚
"""

import json
import re
from collections import defaultdict, Counter
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
import statistics

from .logging_config import get_logger


class LogEntry:
    """æ—¥èªŒæ¢ç›®çµæ§‹åŒ–è¡¨ç¤º"""

    def __init__(self, timestamp: datetime, level: str, message: str,
                 module: str = None, **extra_fields):
        self.timestamp = timestamp
        self.level = level
        self.message = message
        self.module = module
        self.extra_fields = extra_fields

    @classmethod
    def from_json_line(cls, line: str) -> Optional['LogEntry']:
        """å¾ JSON æ—¥èªŒè¡Œå»ºç«‹ LogEntry"""
        try:
            data = json.loads(line)
            timestamp = datetime.fromisoformat(data.get('timestamp', '').replace('Z', '+00:00'))
            level = data.get('level', 'INFO')
            message = data.get('message', '')
            module = data.get('module')

            # æå–å…¶ä»–æ¬„ä½
            extra_fields = {k: v for k, v in data.items()
                          if k not in ['timestamp', 'level', 'message', 'module']}

            return cls(timestamp, level, message, module, **extra_fields)
        except (json.JSONDecodeError, ValueError, KeyError) as e:
            return None

    def to_dict(self) -> Dict[str, Any]:
        """è½‰æ›ç‚ºå­—å…¸"""
        result = {
            'timestamp': self.timestamp.isoformat(),
            'level': self.level,
            'message': self.message,
        }
        if self.module:
            result['module'] = self.module
        result.update(self.extra_fields)
        return result


class LogPattern:
    """æ—¥èªŒæ¨¡å¼å®šç¾©"""

    def __init__(self, name: str, pattern: str, severity: str = 'info',
                 description: str = '', threshold: int = 1):
        self.name = name
        self.pattern = re.compile(pattern, re.IGNORECASE)
        self.severity = severity  # info, warning, error, critical
        self.description = description
        self.threshold = threshold  # è§¸ç™¼è­¦å‘Šçš„æ¬¡æ•¸é–¾å€¼
        self.matches = []

    def match(self, log_entry: LogEntry) -> bool:
        """æª¢æŸ¥æ—¥èªŒæ¢ç›®æ˜¯å¦åŒ¹é…æ¨¡å¼"""
        if self.pattern.search(log_entry.message):
            self.matches.append(log_entry)
            return True
        return False

    def is_triggered(self) -> bool:
        """æª¢æŸ¥æ˜¯å¦è§¸ç™¼è­¦å‘Šé–¾å€¼"""
        return len(self.matches) >= self.threshold

    def reset(self):
        """é‡ç½®åŒ¹é…è¨˜éŒ„"""
        self.matches.clear()


class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ¨™æ”¶é›†å™¨"""

    def __init__(self):
        self.operation_times = defaultdict(list)
        self.error_counts = Counter()
        self.success_counts = Counter()
        self.start_times = {}

    def record_operation_start(self, operation: str, operation_id: str = None):
        """è¨˜éŒ„æ“ä½œé–‹å§‹æ™‚é–“"""
        key = f"{operation}:{operation_id}" if operation_id else operation
        self.start_times[key] = datetime.now()

    def record_operation_end(self, operation: str, operation_id: str = None,
                           success: bool = True):
        """è¨˜éŒ„æ“ä½œçµæŸæ™‚é–“"""
        key = f"{operation}:{operation_id}" if operation_id else operation

        if key in self.start_times:
            duration = (datetime.now() - self.start_times[key]).total_seconds()
            self.operation_times[operation].append(duration)
            del self.start_times[key]

            if success:
                self.success_counts[operation] += 1
            else:
                self.error_counts[operation] += 1

    def get_operation_stats(self, operation: str) -> Dict[str, Any]:
        """å–å¾—æ“ä½œçµ±è¨ˆ"""
        times = self.operation_times.get(operation, [])
        if not times:
            return {
                'operation': operation,
                'count': 0,
                'success_rate': 0.0,
                'avg_duration': 0.0
            }

        return {
            'operation': operation,
            'count': len(times),
            'success_count': self.success_counts[operation],
            'error_count': self.error_counts[operation],
            'success_rate': self.success_counts[operation] /
                          (self.success_counts[operation] + self.error_counts[operation])
                          if (self.success_counts[operation] + self.error_counts[operation]) > 0 else 0.0,
            'avg_duration': statistics.mean(times),
            'min_duration': min(times),
            'max_duration': max(times),
            'median_duration': statistics.median(times),
            'p95_duration': self._percentile(times, 95) if len(times) >= 5 else max(times)
        }

    def _percentile(self, data: List[float], percentile: int) -> float:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]


class LogAnalyzer:
    """æ—¥èªŒåˆ†æä¸»é¡åˆ¥"""

    def __init__(self, log_dir: Union[str, Path] = "logs"):
        self.logger = get_logger("log_analyzer")
        self.log_dir = Path(log_dir)
        self.patterns = self._initialize_patterns()
        self.metrics = PerformanceMetrics()
        self.analysis_cache = {}

    def _initialize_patterns(self) -> List[LogPattern]:
        """åˆå§‹åŒ–é è¨­æ—¥èªŒæ¨¡å¼"""
        return [
            # éŒ¯èª¤æ¨¡å¼
            LogPattern(
                "login_failures",
                r"ç™»å…¥å¤±æ•—|login.*failed|authentication.*failed",
                "error",
                "ç™»å…¥å¤±æ•—æª¢æ¸¬",
                threshold=3
            ),
            LogPattern(
                "navigation_errors",
                r"å°èˆª.*å¤±æ•—|navigation.*failed|element.*not.*found",
                "warning",
                "å°èˆªéŒ¯èª¤æª¢æ¸¬",
                threshold=5
            ),
            LogPattern(
                "timeout_errors",
                r"timeout|è¶…æ™‚|ç­‰å¾….*è¶…æ™‚",
                "warning",
                "è¶…æ™‚éŒ¯èª¤æª¢æ¸¬",
                threshold=3
            ),
            LogPattern(
                "critical_errors",
                r"critical|fatal|crashed|å´©æ½°",
                "critical",
                "åš´é‡éŒ¯èª¤æª¢æ¸¬",
                threshold=1
            ),

            # æ€§èƒ½æ¨¡å¼
            LogPattern(
                "slow_operations",
                r"duration.*[5-9]\d{3,}|åŸ·è¡Œæ™‚é–“.*[5-9]\d+ç§’",
                "warning",
                "æ…¢æ“ä½œæª¢æ¸¬",
                threshold=2
            ),

            # æˆåŠŸæ¨¡å¼
            LogPattern(
                "successful_downloads",
                r"ä¸‹è¼‰.*æˆåŠŸ|å·²ç”Ÿæˆ.*Excel|successfully.*downloaded",
                "info",
                "æˆåŠŸä¸‹è¼‰è¿½è¹¤"
            ),
        ]

    def add_pattern(self, pattern: LogPattern):
        """æ·»åŠ è‡ªå®šç¾©æ¨¡å¼"""
        self.patterns.append(pattern)

    def analyze_log_file(self, log_file: Path,
                        time_range: Optional[Tuple[datetime, datetime]] = None) -> Dict[str, Any]:
        """åˆ†æå–®å€‹æ—¥èªŒæª”æ¡ˆ"""
        if not log_file.exists():
            self.logger.warning(f"æ—¥èªŒæª”æ¡ˆä¸å­˜åœ¨: {log_file}")
            return {}

        # æª¢æŸ¥å¿«å–
        cache_key = f"{log_file}_{log_file.stat().st_mtime}"
        if cache_key in self.analysis_cache:
            return self.analysis_cache[cache_key]

        entries = []
        invalid_lines = 0

        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if not line:
                        continue

                    entry = LogEntry.from_json_line(line)
                    if entry:
                        # æª¢æŸ¥æ™‚é–“ç¯„åœ
                        if time_range:
                            start_time, end_time = time_range
                            if not (start_time <= entry.timestamp <= end_time):
                                continue
                        entries.append(entry)
                    else:
                        invalid_lines += 1

        except Exception as e:
            self.logger.error(f"è®€å–æ—¥èªŒæª”æ¡ˆå¤±æ•— {log_file}: {e}")
            return {}

        # åˆ†ææ—¥èªŒæ¢ç›®
        analysis = self._analyze_entries(entries)
        analysis['file_info'] = {
            'path': str(log_file),
            'total_entries': len(entries),
            'invalid_lines': invalid_lines,
            'size_bytes': log_file.stat().st_size,
        }

        # å¿«å–çµæœ
        self.analysis_cache[cache_key] = analysis
        return analysis

    def analyze_directory(self, time_range: Optional[Tuple[datetime, datetime]] = None,
                         file_pattern: str = "*.json") -> Dict[str, Any]:
        """åˆ†ææ•´å€‹æ—¥èªŒç›®éŒ„"""
        log_files = list(self.log_dir.glob(file_pattern))
        if not log_files:
            self.logger.warning(f"åœ¨ {self.log_dir} ä¸­æ‰¾ä¸åˆ°ç¬¦åˆ {file_pattern} çš„æ—¥èªŒæª”æ¡ˆ")
            return {}

        # æŒ‰ä¿®æ”¹æ™‚é–“æ’åº
        log_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)

        all_entries = []
        file_analyses = {}

        for log_file in log_files:
            file_analysis = self.analyze_log_file(log_file, time_range)
            if file_analysis:
                file_analyses[str(log_file)] = file_analysis
                # æ”¶é›†æ‰€æœ‰æ¢ç›®é€²è¡Œæ•´é«”åˆ†æ
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        for line in f:
                            line = line.strip()
                            if line:
                                entry = LogEntry.from_json_line(line)
                                if entry:
                                    if time_range:
                                        start_time, end_time = time_range
                                        if start_time <= entry.timestamp <= end_time:
                                            all_entries.append(entry)
                                    else:
                                        all_entries.append(entry)
                except Exception as e:
                    self.logger.warning(f"è™•ç†æª”æ¡ˆ {log_file} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")

        # æ•´é«”åˆ†æ
        overall_analysis = self._analyze_entries(all_entries)
        overall_analysis['file_analyses'] = file_analyses
        overall_analysis['analyzed_files'] = len(log_files)

        return overall_analysis

    def _analyze_entries(self, entries: List[LogEntry]) -> Dict[str, Any]:
        """åˆ†ææ—¥èªŒæ¢ç›®åˆ—è¡¨"""
        if not entries:
            return {}

        # é‡ç½®æ¨¡å¼åŒ¹é…
        for pattern in self.patterns:
            pattern.reset()

        # åŸºæœ¬çµ±è¨ˆ
        level_counts = Counter(entry.level for entry in entries)
        module_counts = Counter(entry.module for entry in entries if entry.module)
        hourly_counts = defaultdict(int)

        # æ¨¡å¼åŒ¹é…å’Œæ™‚é–“åˆ†å¸ƒ
        triggered_patterns = []
        for entry in entries:
            # æª¢æŸ¥æ¨¡å¼
            for pattern in self.patterns:
                if pattern.match(entry):
                    continue  # æ¨¡å¼å·²åœ¨ match æ–¹æ³•ä¸­è¨˜éŒ„

            # æ™‚é–“åˆ†å¸ƒçµ±è¨ˆ
            hour_key = entry.timestamp.strftime('%Y-%m-%d %H:00')
            hourly_counts[hour_key] += 1

            # æ€§èƒ½æŒ‡æ¨™æå–
            self._extract_performance_metrics(entry)

        # æª¢æŸ¥è§¸ç™¼çš„æ¨¡å¼
        for pattern in self.patterns:
            if pattern.is_triggered():
                triggered_patterns.append({
                    'name': pattern.name,
                    'severity': pattern.severity,
                    'description': pattern.description,
                    'match_count': len(pattern.matches),
                    'threshold': pattern.threshold,
                    'recent_matches': [
                        {
                            'timestamp': match.timestamp.isoformat(),
                            'message': match.message
                        }
                        for match in pattern.matches[-5:]  # æœ€è¿‘ 5 å€‹åŒ¹é…
                    ]
                })

        # ç•°å¸¸æª¢æ¸¬
        anomalies = self._detect_anomalies(entries)

        # æ€§èƒ½åˆ†æ
        performance_summary = self._analyze_performance()

        return {
            'summary': {
                'total_entries': len(entries),
                'time_range': {
                    'start': min(entries, key=lambda e: e.timestamp).timestamp.isoformat(),
                    'end': max(entries, key=lambda e: e.timestamp).timestamp.isoformat(),
                } if entries else None,
                'level_distribution': dict(level_counts),
                'module_distribution': dict(module_counts),
            },
            'triggered_patterns': triggered_patterns,
            'anomalies': anomalies,
            'performance': performance_summary,
            'time_distribution': dict(hourly_counts),
            'analysis_timestamp': datetime.now().isoformat(),
        }

    def _extract_performance_metrics(self, entry: LogEntry):
        """å¾æ—¥èªŒæ¢ç›®æå–æ€§èƒ½æŒ‡æ¨™"""
        # æª¢æŸ¥æ“ä½œæˆåŠŸ/å¤±æ•—
        if 'æˆåŠŸ' in entry.message or 'success' in entry.message.lower():
            # å˜—è©¦æå–æ“ä½œé¡å‹
            if 'operation' in entry.extra_fields:
                operation = entry.extra_fields['operation']
                self.metrics.success_counts[operation] += 1

        elif 'å¤±æ•—' in entry.message or 'failed' in entry.message.lower() or 'error' in entry.message.lower():
            if 'operation' in entry.extra_fields:
                operation = entry.extra_fields['operation']
                self.metrics.error_counts[operation] += 1

        # æå–æŒçºŒæ™‚é–“
        if 'duration' in entry.extra_fields:
            try:
                duration = float(entry.extra_fields['duration'])
                operation = entry.extra_fields.get('operation', 'unknown')
                self.metrics.operation_times[operation].append(duration)
            except (ValueError, TypeError):
                pass

    def _detect_anomalies(self, entries: List[LogEntry]) -> List[Dict[str, Any]]:
        """ç•°å¸¸æª¢æ¸¬"""
        anomalies = []

        if len(entries) < 10:  # æ•¸æ“šä¸è¶³ï¼Œç„¡æ³•é€²è¡Œç•°å¸¸æª¢æ¸¬
            return anomalies

        # æ™‚é–“é–“éš”ç•°å¸¸æª¢æ¸¬
        time_intervals = []
        for i in range(1, len(entries)):
            interval = (entries[i].timestamp - entries[i-1].timestamp).total_seconds()
            time_intervals.append(interval)

        if time_intervals:
            avg_interval = statistics.mean(time_intervals)
            std_interval = statistics.stdev(time_intervals) if len(time_intervals) > 1 else 0

            # æª¢æ¸¬ç•°å¸¸é–“éš”ï¼ˆè¶…éå¹³å‡å€¼ + 3*æ¨™æº–å·®ï¼‰
            threshold = avg_interval + 3 * std_interval
            for i, interval in enumerate(time_intervals):
                if interval > threshold and interval > 300:  # è¶…é 5 åˆ†é˜ä¸”ç•°å¸¸
                    anomalies.append({
                        'type': 'time_gap',
                        'description': f'æ—¥èªŒæ™‚é–“é–“éš”ç•°å¸¸ï¼š{interval:.1f}ç§’',
                        'timestamp': entries[i+1].timestamp.isoformat(),
                        'severity': 'warning',
                        'details': {
                            'interval_seconds': interval,
                            'average_interval': avg_interval,
                            'threshold': threshold
                        }
                    })

        # éŒ¯èª¤çˆ†ç™¼æª¢æ¸¬
        error_entries = [e for e in entries if e.level in ['ERROR', 'CRITICAL']]
        if len(error_entries) > 5:  # è‡³å°‘ 5 å€‹éŒ¯èª¤æ‰æª¢æ¸¬
            # æª¢æŸ¥ 5 åˆ†é˜å…§çš„éŒ¯èª¤å¯†åº¦
            window_size = timedelta(minutes=5)
            for i, entry in enumerate(error_entries[:-2]):  # è‡³å°‘éœ€è¦ 3 å€‹éŒ¯èª¤
                window_end = entry.timestamp + window_size
                errors_in_window = sum(1 for e in error_entries[i:]
                                     if e.timestamp <= window_end)

                if errors_in_window >= 5:  # 5 åˆ†é˜å…§ 5 å€‹éŒ¯èª¤
                    anomalies.append({
                        'type': 'error_burst',
                        'description': f'éŒ¯èª¤çˆ†ç™¼ï¼š5åˆ†é˜å…§{errors_in_window}å€‹éŒ¯èª¤',
                        'timestamp': entry.timestamp.isoformat(),
                        'severity': 'error',
                        'details': {
                            'error_count': errors_in_window,
                            'window_minutes': 5
                        }
                    })
                    break  # åªå ±å‘Šç¬¬ä¸€å€‹çˆ†ç™¼

        return anomalies

    def _analyze_performance(self) -> Dict[str, Any]:
        """åˆ†ææ€§èƒ½æŒ‡æ¨™"""
        performance_data = {}

        for operation in self.metrics.operation_times.keys():
            stats = self.metrics.get_operation_stats(operation)
            performance_data[operation] = stats

        # æ•´é«”çµ±è¨ˆ
        all_operations = list(self.metrics.operation_times.keys())
        total_successes = sum(self.metrics.success_counts.values())
        total_errors = sum(self.metrics.error_counts.values())

        return {
            'operations': performance_data,
            'summary': {
                'total_operations': len(all_operations),
                'total_successes': total_successes,
                'total_errors': total_errors,
                'overall_success_rate': total_successes / (total_successes + total_errors)
                                      if (total_successes + total_errors) > 0 else 0.0
            }
        }

    def generate_report(self, time_range: Optional[Tuple[datetime, datetime]] = None,
                       output_format: str = 'json') -> str:
        """ç”Ÿæˆåˆ†æå ±å‘Š"""
        analysis = self.analyze_directory(time_range)

        if output_format.lower() == 'json':
            return json.dumps(analysis, indent=2, ensure_ascii=False, default=str)

        elif output_format.lower() == 'markdown':
            return self._generate_markdown_report(analysis)

        else:
            raise ValueError(f"ä¸æ”¯æ´çš„è¼¸å‡ºæ ¼å¼: {output_format}")

    def _generate_markdown_report(self, analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆ Markdown æ ¼å¼å ±å‘Š"""
        lines = [
            "# æ—¥èªŒåˆ†æå ±å‘Š",
            "",
            f"**åˆ†ææ™‚é–“**: {analysis.get('analysis_timestamp', '')}",
            "",
        ]

        # æ‘˜è¦
        summary = analysis.get('summary', {})
        if summary:
            lines.extend([
                "## æ‘˜è¦",
                "",
                f"- **ç¸½æ¢ç›®æ•¸**: {summary.get('total_entries', 0):,}",
                f"- **åˆ†ææª”æ¡ˆæ•¸**: {analysis.get('analyzed_files', 0)}",
                "",
            ])

            # æ™‚é–“ç¯„åœ
            time_range = summary.get('time_range')
            if time_range:
                lines.extend([
                    f"- **æ™‚é–“ç¯„åœ**: {time_range['start']} è‡³ {time_range['end']}",
                    "",
                ])

            # å±¤ç´šåˆ†å¸ƒ
            level_dist = summary.get('level_distribution', {})
            if level_dist:
                lines.extend([
                    "### æ—¥èªŒå±¤ç´šåˆ†å¸ƒ",
                    "",
                ])
                for level, count in sorted(level_dist.items()):
                    lines.append(f"- **{level}**: {count:,}")
                lines.append("")

        # è§¸ç™¼çš„æ¨¡å¼
        triggered = analysis.get('triggered_patterns', [])
        if triggered:
            lines.extend([
                "## è§¸ç™¼çš„è­¦å‘Šæ¨¡å¼",
                "",
            ])
            for pattern in triggered:
                severity_emoji = {
                    'critical': 'ğŸ”´',
                    'error': 'âŒ',
                    'warning': 'âš ï¸',
                    'info': 'â„¹ï¸'
                }.get(pattern['severity'], 'â€¢')

                lines.extend([
                    f"### {severity_emoji} {pattern['name']}",
                    "",
                    f"- **æè¿°**: {pattern['description']}",
                    f"- **åš´é‡æ€§**: {pattern['severity']}",
                    f"- **åŒ¹é…æ¬¡æ•¸**: {pattern['match_count']} (é–¾å€¼: {pattern['threshold']})",
                    "",
                ])

                if pattern.get('recent_matches'):
                    lines.extend([
                        "**æœ€è¿‘åŒ¹é…**:",
                        "",
                    ])
                    for match in pattern['recent_matches']:
                        lines.append(f"- `{match['timestamp']}`: {match['message']}")
                    lines.append("")

        # ç•°å¸¸æª¢æ¸¬
        anomalies = analysis.get('anomalies', [])
        if anomalies:
            lines.extend([
                "## æª¢æ¸¬åˆ°çš„ç•°å¸¸",
                "",
            ])
            for anomaly in anomalies:
                severity_emoji = {
                    'critical': 'ğŸ”´',
                    'error': 'âŒ',
                    'warning': 'âš ï¸'
                }.get(anomaly['severity'], 'â€¢')

                lines.extend([
                    f"### {severity_emoji} {anomaly['type']}",
                    "",
                    f"- **æè¿°**: {anomaly['description']}",
                    f"- **æ™‚é–“**: {anomaly['timestamp']}",
                    f"- **åš´é‡æ€§**: {anomaly['severity']}",
                    "",
                ])

        # æ€§èƒ½åˆ†æ
        performance = analysis.get('performance', {})
        if performance and performance.get('operations'):
            lines.extend([
                "## æ€§èƒ½åˆ†æ",
                "",
            ])

            perf_summary = performance.get('summary', {})
            if perf_summary:
                lines.extend([
                    f"- **ç¸½æˆåŠŸæ“ä½œ**: {perf_summary.get('total_successes', 0):,}",
                    f"- **ç¸½å¤±æ•—æ“ä½œ**: {perf_summary.get('total_errors', 0):,}",
                    f"- **æ•´é«”æˆåŠŸç‡**: {perf_summary.get('overall_success_rate', 0):.1%}",
                    "",
                ])

            operations = performance.get('operations', {})
            if operations:
                lines.extend([
                    "### æ“ä½œè©³ç´°çµ±è¨ˆ",
                    "",
                    "| æ“ä½œ | æ¬¡æ•¸ | æˆåŠŸç‡ | å¹³å‡æ™‚é–“ | æœ€çŸ­æ™‚é–“ | æœ€é•·æ™‚é–“ |",
                    "|------|------|--------|----------|----------|----------|",
                ])

                for op_name, stats in operations.items():
                    lines.append(
                        f"| {op_name} | {stats['count']:,} | "
                        f"{stats['success_rate']:.1%} | "
                        f"{stats['avg_duration']:.2f}s | "
                        f"{stats['min_duration']:.2f}s | "
                        f"{stats['max_duration']:.2f}s |"
                    )
                lines.append("")

        return "\n".join(lines)

    def monitor_realtime(self, callback_func, check_interval: int = 30):
        """å³æ™‚ç›£æ§æ—¥èªŒï¼ˆç¤ºä¾‹å¯¦ä½œï¼‰"""
        # é€™æ˜¯ä¸€å€‹ç¤ºä¾‹æ–¹æ³•ï¼Œå¯¦éš›å¯¦ä½œå¯èƒ½éœ€è¦ä½¿ç”¨æª”æ¡ˆç›£æ§åº«å¦‚ watchdog
        last_analysis_time = datetime.now()

        while True:
            try:
                # åˆ†ææœ€è¿‘çš„æ—¥èªŒ
                current_time = datetime.now()
                time_range = (last_analysis_time, current_time)

                analysis = self.analyze_directory(time_range)

                # æª¢æŸ¥è§¸ç™¼çš„æ¨¡å¼å’Œç•°å¸¸
                alerts = []
                for pattern in analysis.get('triggered_patterns', []):
                    if pattern['severity'] in ['error', 'critical']:
                        alerts.append(pattern)

                for anomaly in analysis.get('anomalies', []):
                    alerts.append(anomaly)

                # å‘¼å«å›èª¿å‡½æ•¸
                if alerts:
                    callback_func(alerts)

                last_analysis_time = current_time

                # ç­‰å¾…ä¸‹æ¬¡æª¢æŸ¥
                import time
                time.sleep(check_interval)

            except KeyboardInterrupt:
                self.logger.info("å³æ™‚ç›£æ§å·²åœæ­¢")
                break
            except Exception as e:
                self.logger.error(f"å³æ™‚ç›£æ§éŒ¯èª¤: {e}")
                import time
                time.sleep(check_interval)


def create_monitoring_dashboard_data(analyzer: LogAnalyzer,
                                   hours_back: int = 24) -> Dict[str, Any]:
    """å»ºç«‹ç›£æ§å„€è¡¨æ¿æ•¸æ“š"""
    end_time = datetime.now()
    start_time = end_time - timedelta(hours=hours_back)

    analysis = analyzer.analyze_directory((start_time, end_time))

    # ç‚ºå„€è¡¨æ¿æº–å‚™æ•¸æ“š
    dashboard_data = {
        'status': 'healthy',  # healthy, warning, error
        'last_updated': end_time.isoformat(),
        'time_range_hours': hours_back,
        'metrics': {
            'total_logs': analysis.get('summary', {}).get('total_entries', 0),
            'error_rate': 0.0,
            'warning_count': 0,
            'critical_alerts': 0,
        },
        'trends': {},
        'alerts': [],
        'performance': analysis.get('performance', {}),
    }

    # è¨ˆç®—ç‹€æ…‹å’ŒæŒ‡æ¨™
    level_dist = analysis.get('summary', {}).get('level_distribution', {})
    total_logs = sum(level_dist.values()) if level_dist else 0
    error_logs = level_dist.get('ERROR', 0) + level_dist.get('CRITICAL', 0)

    if total_logs > 0:
        dashboard_data['metrics']['error_rate'] = error_logs / total_logs

    # è­¦å‘Šå’Œåš´é‡å•é¡Œ
    triggered_patterns = analysis.get('triggered_patterns', [])
    critical_count = sum(1 for p in triggered_patterns if p['severity'] == 'critical')
    warning_count = sum(1 for p in triggered_patterns if p['severity'] in ['warning', 'error'])

    dashboard_data['metrics']['warning_count'] = warning_count
    dashboard_data['metrics']['critical_alerts'] = critical_count

    # æ±ºå®šæ•´é«”ç‹€æ…‹
    if critical_count > 0:
        dashboard_data['status'] = 'error'
    elif warning_count > 0 or dashboard_data['metrics']['error_rate'] > 0.1:
        dashboard_data['status'] = 'warning'

    # æ´»èºè­¦å‘Š
    dashboard_data['alerts'] = triggered_patterns + analysis.get('anomalies', [])

    return dashboard_data


if __name__ == "__main__":
    # æ¸¬è©¦ç¨‹å¼ç¢¼
    analyzer = LogAnalyzer("logs")

    # åˆ†ææœ€è¿‘ 24 å°æ™‚çš„æ—¥èªŒ
    end_time = datetime.now()
    start_time = end_time - timedelta(hours=24)

    report = analyzer.generate_report((start_time, end_time), 'markdown')
    print(report)